rpart1
head(training)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
head(olive)
newdata
str(olive)
summary(olive)
summary(olive)
newdata
data(olive)
head(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=TRUE)
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
dim(training); dim(testing)
qplot(Palmatic, Palmitoleic, colour=Area, data=training)
qplot(Palmitic, Palmitoleic, colour=Area, data=training)
modFit <- train(Area ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.6)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.6)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=2)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
predict(modFit, newdata=newdata)
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
dim(training); dim(testing)
modFit <- train(Area ~ ., method="rpart", data=training)
print(modFit$finalModel)
# Plot tree
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
# Predicting new values
predict(modFit, newdata=newdata)
library(pgmm); library(caret); set.seed(125)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
dim(training); dim(testing)
modFit <- train(Area ~ ., method="rpart", data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=0.8)
predict(modFit, newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
head(SAheart)
sample(1:10,size=2,replace = F)
sample(1:10,size=5,replace = F)
set.seed(13234)
fitMod <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, family="binomial")
prediction <- predict(fitMod, newdata=testSA)
values <- testSA$chd
missClass = function(values,prediction){sum(((prediction > 0.5)*1)
!= values)/length(values)}
missClass()
missClass(values, prediction)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
fitMod <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, family="binomial")
pred.train <- predict(fitMod, newdata=trainSA)
pred.test <- predict(fitMod, newdata=testSA)
val.train <- trainSA$chd
val.test <- testSA$chd
missClass = function(values, prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(val.train, pred.train)
missClass(val.test, pred.test)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
fitMod.train <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, family="binomial")
fitMod.test <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=testSA, family="binomial")
pred.train <- predict(fitMod.train, newdata=trainSA)
pred.test <- predict(fitMod.test, newdata=testSA)
val.train <- trainSA$chd
val.test <- testSA$chd
missClass = function(values, prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(val.train, pred.train)
missClass(val.test, pred.test)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
fitMod <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, family="binomial")
missClass = function(values, prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(fitMod, newdata=trainSA))
missClass(testSA$chd, predict(fitMod, newdata=testSA))
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(1234)
fitMod <- glm(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, family="binomial")
missClass = function(values, prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(fitMod, newdata=trainSA))
missClass(testSA$chd, predict(fitMod, newdata=testSA))
fitMod <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,
data=trainSA, method="glm", family="binomial")
missClass = function(values, prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(fitMod, newdata=trainSA))
missClass(testSA$chd, predict(fitMod, newdata=testSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
head(vowel.test)
head(vowel.train)
library(caret)
modFit <- train(y~ ., data=vowel.train, method="rf", prox=TRUE)
library(ElemStatLearn); library(ggplot2)
library(caret)
modFit <- train(y~ ., data=vowel.train, method="rf", prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
v <- varlmp(modFit)
v <- varImp(modFit)
library(ElemStatLearn); library(ggplot2)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
# Random forests
modelfit <- randomForest(y ~ ., data = vowel.train,
importance = FALSE)
order(varImp(modelfit), decreasing = TRUE)
head(vowel.test)
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
head(training)
str(training)
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
str(training)
?read.csv
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
head(train.set,3)
train.set <- read.csv(train.path, na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, na.strings = c("NA", "#DIV/0!", ""))
head(train.set,3)
dim(train.set)
dim(test.set)
is.na(train.set)
colSums(is.na(train.set))
train.set$skewness_yaw_belt
colSums(is.na(train.set))
(colSums(is.na(train.set)) == 0)
(colSums(is.na(train.set)) == 0)
head(train.set[,colSums(is.na(train.set)) == 0],3)
is.na(train.set)
train.set <- train.set[,colSums(is.na(train.set)) == 0]
is.na(train.set)
sum(is.na(train.set))
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set)
dim(test.set)
head(train.set)
head(test.set)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
# We remove columns that contain NA.
train.set <- train.set[,colSums(is.na(train.set)) == 0]
test.set <- test.set[,colSums(is.na(test.set)) == 0]
# Just to ensure there is no NA left in the data, we sum if there are any,
sum(is.na(train.set))
sum(is.na(test.set))
# We check dimension/columns of the remaining data.
dim(train.set)
dim(test.set)
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
dim(sub.train.set)
dim(sub.test.set)
head(sub.train.set)
head(sub.train.set)
library(caret)
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
dim(sub.train.set)
dim(sub.test.set)
head(sub.train.set)
head(sub.train.set)
table(sub.train.set$classe)
table(sub.test.set$classe)
table(train.set$classe)
?randomForest
library(caret); library(randomForest)
?randomForest
# Initialize libraries.
library(caret); library(randomForest)
# Getting the data
# Some of the data fields contain "#DIV/0!" or are blank (""). These fields will be filled with "NA".
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set); dim(test.set)
head(train.set, 3); head(test.set, 3)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
# We remove columns that contain NA.
train.set <- train.set[,colSums(is.na(train.set)) == 0]
test.set <- test.set[,colSums(is.na(test.set)) == 0]
Initialize libraries.
library(caret); library(randomForest)
# Getting the data
# Some of the data fields contain "#DIV/0!" or are blank (""). These fields will be filled with "NA".
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set); dim(test.set)
head(train.set, 3); head(test.set, 3)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
# We remove columns that contain NA.
train.set <- train.set[,colSums(is.na(train.set)) == 0]
test.set <- test.set[,colSums(is.na(test.set)) == 0]
# Just to ensure there is no NA left in the data, we sum if there are any,
sum(is.na(train.set))
sum(is.na(test.set))
columns of the remaining data.
dim(train.set); dim(test.set)
# Cross-validation
# The training set is split in to 2 subsets in order to conduct cross-validation. The sub.training set will get 75% of the data, while the sub.test set will get the remaining 25% of the data. Assignment will be done using random sampling without replacement.
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
4904+14718
dim(sub.train.set); dim(sub.test.set)
head(sub.train.set, 3); head(sub.train.set, 3)
table(train.set$classe)
table(sub.train.set$classe)
table(sub.test.set$classe)
pred.model <- randomForest(classe ~. , data = sub.train.set, importance = TRUE)
pred.rf <- predict(model.rf, sub.test.set)
pred.rf <- predict(pred.model, sub.test.set)
confusionMatrix(pred.rf, sub.test.set$classe)
# Initialize libraries.
library(caret); library(randomForest)
# Getting the data
# Some of the data fields contain "#DIV/0!" or are blank (""). These fields will be filled with "NA".
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set); dim(test.set)
head(train.set, 3); head(test.set, 3)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
sum(is.na(train.set))
sum(is.na(test.set))
# Cross-validation
# The training set is split in to 2 subsets in order to conduct cross-validation. The sub.training set will get 75% of the data, while the sub.test set will get the remaining 25% of the data. Assignment will be done using random sampling without replacement.
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
model.rf <- randomForest(classe ~. , data = sub.train.set, importance = TRUE)
model.rf <- randomForest(classe ~. , data = sub.train.set, importance = TRUE, na.action = na.omit)
# Initialize libraries.
library(caret); library(randomForest)
# Getting the data
# Some of the data fields contain "#DIV/0!" or are blank (""). These fields will be filled with "NA".
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set); dim(test.set)
head(train.set, 3); head(test.set, 3)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
dim(train.set); dim(test.set)
# We remove columns that contain NA.
train.set <- train.set[,colSums(is.na(train.set)) == 0]
test.set <- test.set[,colSums(is.na(test.set)) == 0]
# Just to ensure there is no NA left in the data, we sum if there are any,
sum(is.na(train.set))
sum(is.na(test.set))
# We check dimension/columns of the remaining data.
dim(train.set); dim(test.set)
# Cross-validation
# The training set is split in to 2 subsets in order to conduct cross-validation. The sub.training set will get 75% of the data, while the sub.test set will get the remaining 25% of the data. Assignment will be done using random sampling without replacement.
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
dim(sub.train.set); dim(sub.test.set)
head(sub.train.set, 3); head(sub.train.set, 3)
# Let's review the variable "classe". The three tables show how the levels are distributed.
table(train.set$classe)
table(sub.train.set$classe)
table(sub.test.set$classe)
# Random Forest - Random Forest package
# Model build
model.rfp <- randomForest(classe ~. , data = sub.train.set, importance = TRUE)
# Prediction
pred.rfp <- predict(model.rfp, sub.test.set)
# Test results on subTesting data set:
confusionMatrix(pred.rfp, sub.test.set$classe)
# Random Forest - Caret package
# Model build
model.rf <- train(classe ~. , data = sub.train.set, method = 'rf')
# Prediction Model - Random Forest
# Model build
model.rf <- randomForest(classe ~. , data = sub.train.set, importance = TRUE)
# Prediction
pred.rfp <- predict(model.rfp, sub.test.set)
# Test results on subTesting data set:
confusionMatrix(pred.rfp, sub.test.set$classe)$table
confusionMatrix(pred.rfp, sub.test.set$classe)$overall[1]
pected, the model performs very well against the training set.
# Next, we will cross validate the model performance against the subset that was not used in the model training.
# Cross validation set accuracy.
pred.rf.sub.test <- predict(model.rf, sub.test.set)
# Test results on sub.test data set:
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$table
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$overall
# The cross validation accuracy is 99.7%. This makes the out-of-sample error equal to 0.3%.
# Test set prediction.
# The model predicts the following classe levels for the test set.
pred.rf.test <- predict(model.rf, set)
pred.rf.test
# As expected, the model performs very well against the training set.
# Next, we will cross validate the model performance against the subset that was not used in the model training.
# Cross validation set accuracy.
pred.rf.sub.test <- predict(model.rf, sub.test.set)
# Test results on sub.test data set:
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$table
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$overall
# The cross validation accuracy is 99.7%. This makes the out-of-sample error equal to 0.3%.
# Test set prediction.
# The model predicts the following classe levels for the test set.
pred.rf.test <- predict(model.rf, set)
pred.rf.test
pred.rf.test <- predict(model.rf, test)
pred.rf.test
pred.rf.test <- predict(model.rf, test.set)
pred.rf.test
# Initialize libraries.
library(caret); library(randomForest)
# Getting the data
# Some of the data fields contain "#DIV/0!" or are blank (""). These fields will be filled with "NA".
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
dim(train.set); dim(test.set)
head(train.set, 3); head(test.set, 3)
# We see that some variables are not relevant for the analysis we are asked to do. These - non-relevant - variables are X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, and num_window. These are the first 7 columns.
train.set <- train.set[ , -c(1:7)]
test.set <- test.set[ , -c(1:7)]
dim(train.set); dim(test.set)
# We remove columns that contain NA.
train.set <- train.set[,colSums(is.na(train.set)) == 0]
test.set <- test.set[,colSums(is.na(test.set)) == 0]
# Just to ensure there is no NA left in the data, we sum if there are any,
sum(is.na(train.set))
sum(is.na(test.set))
# We check dimension/columns of the remaining data.
dim(train.set); dim(test.set)
# Cross-validation
# The training set is split in to 2 subsets in order to conduct cross-validation. The sub.training set will get 75% of the data, while the sub.test set will get the remaining 25% of the data. Assignment will be done using random sampling without replacement.
sub.set <- createDataPartition(y = train.set$classe, p=0.75, list=FALSE)
sub.train.set <- train.set[ sub.set, ]
sub.test.set <- train.set[ -sub.set, ]
dim(sub.train.set); dim(sub.test.set)
head(sub.train.set, 3); head(sub.train.set, 3)
# Let's review the variable "classe". The three tables show how the levels are distributed.
table(train.set$classe)
table(sub.train.set$classe)
table(sub.test.set$classe)
# Prediction Model - Random Forest
# Model Training
# We use the randomForest function from the same named 'randomForest' package.
model.rf <- randomForest(classe ~. , data = sub.train.set, importance = TRUE)
# Model Validation
# We will review how the model performs on the training set itself and the cross validation set.
# Training set accuracy.
pred.rf.sub.train <- predict(model.rf, sub.train.set)
# Test results on sub.train data set:
confusionMatrix(pred.rf.sub.train, sub.train.set$classe)$table
confusionMatrix(pred.rf.sub.train, sub.train.set$classe)$overall
# As expected, the model performs very well against the training set.
# Next, we will cross validate the model performance against the subset that was not used in the model training.
# Cross validation set accuracy.
pred.rf.sub.test <- predict(model.rf, sub.test.set)
# Test results on sub.test data set:
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$table
confusionMatrix(pred.rf.sub.test, sub.test.set$classe)$overall
# The cross validation accuracy is 99.7%. This makes the out-of-sample error equal to 0.3%.
# Test set prediction.
# The model predicts the following classe levels for the test set.
pred.rf.test <- predict(model.rf, test.set)
pred.rf.test
dir()
train.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test.path <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.set <- read.csv(train.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
test.set <- read.csv(test.path, head=TRUE, sep=',', na.strings = c("NA", "#DIV/0!", ""))
install.packages("leaflet")
?mean
?dgamma
?sd
?lm
?colSums
?predict
?show
show(q)
q<-1
show(q)
install.packages("plot;y")
install.packages("plotly")
?suppressPackageStartu
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
library(plotly)
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
version
library(dplyr)
library(tidyr)
install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
library(dplyr)
install.packages("tidyr")
install.packages("ggplot")
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
load("~/Dropbox/Bieb/2018 05 28 - DS - Spec - Developing Data Products  - 1.1 - Shiny.pdf")
shiny::runApp('Documents/Github Repos/DataScienceSpecialisation/9. Developing Data Products/Final assignment')
runApp('Documents/Github Repos/DataScienceSpecialisation/9. Developing Data Products/Final assignment')
runApp()
setwd("~/Documents/Github Repos/DataScienceSpecialisation/9. Developing Data Products")
setwd("~/Documents/Github Repos/DataScienceSpecialisation/9. Developing Data Products/Final assignment")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("rgl")
dat <- replicate(2, 1:3)
dat
plot3d(dat, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
library(rgl)
install.packages("rgl")
library(rgl)
plot3d(dat, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
